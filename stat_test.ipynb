{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'dgl'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (c:\\Users\\kamil\\miniconda3\\envs\\masters\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from rdkit import Chem\n",
    "import scipy.stats as stats\n",
    "\n",
    "from src.dataset import load_cyp_data_split, load_herg_data_split, load_pampa_data_split, load_synthetic_data_split\n",
    "from src.utils import get_data_partition_on_substructure_presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "[16:06:36] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:06:36] WARNING: not removing hydrogen atom without neighbors\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_cyp, valid_cyp, test_cyp = load_cyp_data_split()\n",
    "train_herg, valid_herg, test_herg = load_herg_data_split()\n",
    "train_pampa, valid_pampa, test_pampa = load_pampa_data_split()\n",
    "train_synthetic, valid_synthetic, test_synthetic = load_synthetic_data_split()\n",
    "\n",
    "test_datasets = {\n",
    "    'cyp': test_cyp,\n",
    "    'herg': test_herg,\n",
    "    'pampa': test_pampa,\n",
    "    'synthetic': test_synthetic\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_smiles = json.load(open('results/smiles_grad_cam.json'))\n",
    "saliency_map_smiles = json.load(open('results/smiles_saliency_map.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_results = {}\n",
    "for dataset_name in ['cyp', 'herg', 'pampa', 'synthetic']:\n",
    "    grad_cam_results[dataset_name] = []\n",
    "    for i in range(len(grad_cam_smiles[dataset_name])):\n",
    "        results = []\n",
    "        for smiles in grad_cam_smiles[dataset_name][i]:\n",
    "            has_substr, no_substr = get_data_partition_on_substructure_presence(test_datasets[dataset_name], smiles)\n",
    "            has_substr_y = [int(float(mol.GetProp(\"y\"))) for mol in has_substr]\n",
    "            no_substr_y = [int(float(mol.GetProp(\"y\"))) for mol in no_substr]\n",
    "            chi_test_table = [\n",
    "                [sum(has_substr_y), len(has_substr_y) - sum(has_substr_y)],\n",
    "                [sum(no_substr_y), len(no_substr_y) - sum(no_substr_y)]\n",
    "            ]\n",
    "            try:\n",
    "                results.append(stats.chi2_contingency(chi_test_table, correction=False))\n",
    "            except ValueError:\n",
    "                results.append(None)\n",
    "        grad_cam_results[dataset_name].append(results)\n",
    "\n",
    "\n",
    "saliency_map_results = {}\n",
    "for dataset_name in ['cyp', 'herg', 'pampa', 'synthetic']:\n",
    "    saliency_map_results[dataset_name] = []\n",
    "    for i in range(len(saliency_map_smiles[dataset_name])):\n",
    "        results = []\n",
    "        for smiles in saliency_map_smiles[dataset_name][i]:\n",
    "            has_substr, no_substr = get_data_partition_on_substructure_presence(test_datasets[dataset_name], smiles)\n",
    "            has_substr_y = [int(float(mol.GetProp(\"y\"))) for mol in has_substr]\n",
    "            no_substr_y = [int(float(mol.GetProp(\"y\"))) for mol in no_substr]\n",
    "            chi_test_table = [\n",
    "                [sum(has_substr_y), len(has_substr_y) - sum(has_substr_y)],\n",
    "                [sum(no_substr_y), len(no_substr_y) - sum(no_substr_y)]\n",
    "            ]\n",
    "            try:\n",
    "                results.append(stats.chi2_contingency(chi_test_table, correction=False))\n",
    "            except ValueError:\n",
    "                results.append(None)\n",
    "        saliency_map_results[dataset_name].append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def did_test_pass(result, threshold=0.05):\n",
    "    return result.pvalue < threshold\n",
    "\n",
    "grad_cam_results_pass = {}\n",
    "for dataset_name in ['cyp', 'herg', 'pampa', 'synthetic']:\n",
    "    grad_cam_results_pass[dataset_name] = []\n",
    "    for i in range(len(grad_cam_results[dataset_name])):\n",
    "        results = []\n",
    "        for result in grad_cam_results[dataset_name][i]:\n",
    "            if result is None:\n",
    "                results.append(False)\n",
    "            else:\n",
    "                results.append(did_test_pass(result))\n",
    "        grad_cam_results_pass[dataset_name].append(results)\n",
    "\n",
    "saliency_map_results_pass = {}\n",
    "for dataset_name in ['cyp', 'herg', 'pampa', 'synthetic']:\n",
    "    saliency_map_results_pass[dataset_name] = []\n",
    "    for i in range(len(saliency_map_results[dataset_name])):\n",
    "        results = []\n",
    "        for result in saliency_map_results[dataset_name][i]:\n",
    "            if result is None:\n",
    "                results.append(False)\n",
    "            else:\n",
    "                results.append(did_test_pass(result))\n",
    "        saliency_map_results_pass[dataset_name].append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top_p_values(smiles, explanation_results):\n",
    "    p_values = {}\n",
    "    top_p_values = {}\n",
    "    for dataset_name in ['cyp', 'herg', 'pampa', 'synthetic']:\n",
    "        p_values[dataset_name] = []\n",
    "        smiles_already_checked = []\n",
    "        for i in range(len(explanation_results[dataset_name])):\n",
    "            for j, result in enumerate(explanation_results[dataset_name][i]):\n",
    "                current_smiles = smiles[dataset_name][i][j]\n",
    "                component_size = i + 3\n",
    "                if result is not None and current_smiles not in smiles_already_checked:\n",
    "                    smiles_already_checked.append(current_smiles)\n",
    "                    p_values[dataset_name].append((component_size, j, result.pvalue))\n",
    "        p_values[dataset_name] = sorted(p_values[dataset_name], key=lambda x: x[2])\n",
    "        top_p_values[dataset_name] = p_values[dataset_name][:3]\n",
    "    return top_p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_results(smiles_path, top_p_values):\n",
    "    smiles = json.load(open(smiles_path))\n",
    "\n",
    "    top_mols = {}\n",
    "    for dataset_name in [\"cyp\", \"herg\", \"pampa\", \"synthetic\"]:\n",
    "        top_mols[dataset_name] = []\n",
    "        for comp_size, j, _ in top_p_values[dataset_name]:\n",
    "            i = comp_size - 3\n",
    "            top_mols[dataset_name].append(Chem.MolFromSmiles(smiles[dataset_name][i][j]))\n",
    "\n",
    "    average_targets = {}\n",
    "    for dataset_name in [\"cyp\", \"herg\", \"pampa\", \"synthetic\"]:\n",
    "        average_targets[dataset_name] = []\n",
    "        for comp_size, j, _ in top_p_values[dataset_name]:\n",
    "            i = comp_size - 3\n",
    "            has_substr, no_substr = get_data_partition_on_substructure_presence(test_datasets[dataset_name], smiles[dataset_name][i][j])\n",
    "            has_substr_y = [int(float(mol.GetProp(\"y\"))) for mol in has_substr]\n",
    "            no_substr_y = [int(float(mol.GetProp(\"y\"))) for mol in no_substr]\n",
    "            average_targets[dataset_name].append((sum(has_substr_y) / len(has_substr_y), sum(no_substr_y) / len(no_substr_y)))\n",
    "    return average_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p_values_grad_cam = calculate_top_p_values(grad_cam_smiles, grad_cam_results)\n",
    "average_targets_grad_cam = find_top_results('results/smiles_grad_cam.json', top_p_values_grad_cam)\n",
    "\n",
    "top_p_values_saliency_map = calculate_top_p_values(saliency_map_smiles, saliency_map_results)\n",
    "average_targets_saliency_map = find_top_results('results/smiles_saliency_map.json', top_p_values_saliency_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cyp': [(0.4742943548387097, 0.15767634854771784),\n",
       "  (0.59375, 0.3488499452354874),\n",
       "  (0.49606815203145477, 0.2765957446808511)],\n",
       " 'herg': [(0.8205128205128205, 0.14285714285714285),\n",
       "  (0.8272727272727273, 0.3333333333333333),\n",
       "  (0.8272727272727273, 0.3333333333333333)],\n",
       " 'pampa': [(0.7314814814814815, 0.8862876254180602),\n",
       "  (0.7088607594936709, 0.8780487804878049),\n",
       "  (0.7559055118110236, 0.8857142857142857)],\n",
       " 'synthetic': [(0.5, 0.9130434782608695),\n",
       "  (0.5, 0.8958333333333334),\n",
       "  (1.0, 0.8344827586206897)]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_targets_grad_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cyp': [(0.4742943548387097, 0.15767634854771784),\n",
       "  (0.59375, 0.3488499452354874),\n",
       "  (0.49606815203145477, 0.2765957446808511)],\n",
       " 'herg': [(0.8205128205128205, 0.14285714285714285),\n",
       "  (0.8272727272727273, 0.3333333333333333),\n",
       "  (0.8272727272727273, 0.3333333333333333)],\n",
       " 'pampa': [(0.7314814814814815, 0.8862876254180602),\n",
       "  (0.7051282051282052, 0.878419452887538),\n",
       "  (0.7088607594936709, 0.8780487804878049)],\n",
       " 'synthetic': [(1.0, 0.0), (1.0, 0.5384615384615384), (1.0, 0.808)]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_targets_saliency_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cyp': [(3, 6, 9.15100277677312e-37),\n",
       "  (4, 9, 2.479752058276682e-27),\n",
       "  (3, 7, 5.7467558731475885e-27)],\n",
       " 'herg': [(3, 4, 3.3918134766802585e-08),\n",
       "  (3, 3, 1.7705562195108747e-06),\n",
       "  (4, 1, 1.7705562195108747e-06)],\n",
       " 'pampa': [(3, 4, 0.00013769770127582053),\n",
       "  (3, 6, 0.0001422317676455562),\n",
       "  (3, 7, 0.00018979605363326577)],\n",
       " 'synthetic': [(3, 0, 2.0884875837625387e-45),\n",
       "  (3, 1, 1.2474874357494747e-18),\n",
       "  (6, 8, 5.2278657832674425e-05)]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_p_values_saliency_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset_name_mapping = {\n",
    "    'cyp': 'CYP3A4',\n",
    "    'herg': 'hERG',\n",
    "    'pampa': 'PAMPA',\n",
    "    'synthetic': 'Synthetic'\n",
    "}\n",
    "\n",
    "method_name_mapping = {\n",
    "    'grad_cam': 'GradCAM',\n",
    "    'saliency_map': 'Saliency Map'\n",
    "}\n",
    "\n",
    "def save_plots_of_average_targets(average_targets, method_name):\n",
    "    for dataset_name, values in average_targets.items():\n",
    "        values = sorted(values, key=lambda x: max(x), reverse=True)\n",
    "        has_substr_values = [v[0] for v in values]\n",
    "        no_substr_values = [v[1] for v in values]\n",
    "        \n",
    "        indices = np.arange(len(values))\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.barh(indices, has_substr_values, height=0.4, color='green', label='Has Component', zorder=3)\n",
    "        plt.barh(indices + 0.4, no_substr_values, height=0.4, color='red', label='No Component', zorder=3)\n",
    "        plt.grid(True, axis='x', which=\"both\", linestyle='--', alpha=0.7, zorder=0)\n",
    "        \n",
    "        # Add title with dataset name and method name\n",
    "        plt.title(f\"{dataset_name_mapping[dataset_name]} - {method_name_mapping[method_name]}\", fontsize=22)\n",
    "        \n",
    "        plt.xlabel('Average Target Value', fontsize=20)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.legend(fontsize=14, loc='lower right')\n",
    "        plt.xlim(0, 1.5)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.gca().yaxis.set_visible(False)\n",
    "\n",
    "        plt.savefig(f\"results/stat_test/{dataset_name}_top_avg_target_differences_{method_name}.png\", bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "save_plots_of_average_targets(average_targets_grad_cam, \"grad_cam\")\n",
    "save_plots_of_average_targets(average_targets_saliency_map, \"saliency_map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Draw\n",
    "\n",
    "def draw_molecules(smiles_list, mols_per_row=1):\n",
    "    mols = [Chem.MolFromSmiles(smiles) for smiles in smiles_list]\n",
    "    img = Draw.MolsToGridImage(mols, molsPerRow=mols_per_row, subImgSize=(300, 300))\n",
    "    return img\n",
    "\n",
    "for dataset_name in ['cyp', 'herg', 'pampa', 'synthetic']:\n",
    "    smiles_to_draw = []\n",
    "    for comp_size, smiles_index, _ in top_p_values_grad_cam[dataset_name]:\n",
    "        i = comp_size - 3\n",
    "        smiles = grad_cam_smiles[dataset_name][i][smiles_index]\n",
    "        smiles_to_draw.append(smiles)\n",
    "    img = draw_molecules(smiles_to_draw)\n",
    "    with open(f\"results/stat_test/{dataset_name}_top_3_mols_drawn_grad_cam.png\",'wb+') as img_file:\n",
    "        img_file.write(img.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in ['cyp', 'herg', 'pampa', 'synthetic']:\n",
    "    smiles_to_draw = []\n",
    "    for comp_size, smiles_index, _ in top_p_values_saliency_map[dataset_name]:\n",
    "        i = comp_size - 3\n",
    "        smiles = saliency_map_smiles[dataset_name][i][smiles_index]\n",
    "        smiles_to_draw.append(smiles)\n",
    "    img = draw_molecules(smiles_to_draw)\n",
    "    with open(f\"results/stat_test/{dataset_name}_top_3_mols_drawn_saliency_map.png\",'wb+') as img_file:\n",
    "        img_file.write(img.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cyp, 3 component size: 9\n",
      "cyp, 4 component size: 8\n",
      "cyp, 5 component size: 7\n",
      "cyp, 6 component size: 6\n",
      "cyp, 7 component size: 7\n",
      "cyp, 8 component size: 10\n",
      "herg, 3 component size: 5\n",
      "herg, 4 component size: 5\n",
      "herg, 5 component size: 6\n",
      "herg, 6 component size: 4\n",
      "herg, 7 component size: 3\n",
      "herg, 8 component size: 2\n",
      "pampa, 3 component size: 2\n",
      "pampa, 4 component size: 1\n",
      "pampa, 5 component size: 1\n",
      "pampa, 6 component size: 2\n",
      "pampa, 7 component size: 2\n",
      "pampa, 8 component size: 3\n",
      "synthetic, 3 component size: 1\n",
      "synthetic, 4 component size: 1\n",
      "synthetic, 5 component size: 1\n",
      "synthetic, 6 component size: 3\n",
      "synthetic, 7 component size: 2\n",
      "synthetic, 8 component size: 3\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in grad_cam_results_pass.keys():\n",
    "    for i in range(len(grad_cam_results_pass[dataset_name])):\n",
    "        print(f\"{dataset_name}, {i + 3} component size: {sum(grad_cam_results_pass[dataset_name][i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cyp, 3 component size: 9\n",
      "cyp, 4 component size: 7\n",
      "cyp, 5 component size: 4\n",
      "cyp, 6 component size: 4\n",
      "cyp, 7 component size: 6\n",
      "cyp, 8 component size: 9\n",
      "herg, 3 component size: 6\n",
      "herg, 4 component size: 5\n",
      "herg, 5 component size: 5\n",
      "herg, 6 component size: 6\n",
      "herg, 7 component size: 6\n",
      "herg, 8 component size: 5\n",
      "pampa, 3 component size: 3\n",
      "pampa, 4 component size: 3\n",
      "pampa, 5 component size: 2\n",
      "pampa, 6 component size: 1\n",
      "pampa, 7 component size: 1\n",
      "pampa, 8 component size: 0\n",
      "synthetic, 3 component size: 4\n",
      "synthetic, 4 component size: 5\n",
      "synthetic, 5 component size: 5\n",
      "synthetic, 6 component size: 7\n",
      "synthetic, 7 component size: 5\n",
      "synthetic, 8 component size: 3\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in saliency_map_results_pass.keys():\n",
    "    for i in range(len(saliency_map_results_pass[dataset_name])):\n",
    "        print(f\"{dataset_name}, {i + 3} component size: {sum(saliency_map_results_pass[dataset_name][i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
